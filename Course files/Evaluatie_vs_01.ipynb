{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load models\n",
    "def load_models(model_paths):\n",
    "    \"\"\"Load YOLOv8 models from given paths.\"\"\"\n",
    "    models = {os.path.basename(path): YOLO(path) for path in model_paths}\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inference\n",
    "def perform_inference(model, dataset_path):\n",
    "    \"\"\"Run inference on a dataset and return predictions.\"\"\"\n",
    "    predictions = []\n",
    "    images = []\n",
    "    for image_file in os.listdir(dataset_path):\n",
    "        image_path = os.path.join(dataset_path, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        results = model(image)\n",
    "        predictions.append(results)\n",
    "        images.append(image)\n",
    "    return predictions, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate metrics\n",
    "def evaluate_metrics(predictions, ground_truths, class_names):\n",
    "    \"\"\"Evaluate precision, recall, and mAP per class.\"\"\"\n",
    "    metrics = {cls: {\"precision\": [], \"recall\": [], \"AP\": 0.0} for cls in class_names}\n",
    "\n",
    "    for cls_idx, cls_name in enumerate(class_names):\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        for pred, gt in zip(predictions, ground_truths):\n",
    "            gt_cls = [1 if obj[\"class\"] == cls_idx else 0 for obj in gt]\n",
    "            pred_cls = [d.conf for d in pred if d.cls == cls_idx]\n",
    "\n",
    "            y_true.extend(gt_cls)\n",
    "            y_scores.extend(pred_cls)\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "        ap = average_precision_score(y_true, y_scores)\n",
    "\n",
    "        metrics[cls_name][\"precision\"] = precision\n",
    "        metrics[cls_name][\"recall\"] = recall\n",
    "        metrics[cls_name][\"AP\"] = ap\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize metrics\n",
    "def visualize_metrics(metrics):\n",
    "    \"\"\"Visualize precision-recall curves per class.\"\"\"\n",
    "    for cls, values in metrics.items():\n",
    "        plt.figure()\n",
    "        plt.plot(values[\"recall\"], values[\"precision\"], label=f'{cls} (AP={values[\"AP\"]:.2f})')\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(f\"Precision-Recall Curve for {cls}\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display results summary\n",
    "def display_summary(metrics):\n",
    "    \"\"\"Print and summarize evaluation results.\"\"\"\n",
    "    print(\"\\nEvaluation Summary\\n------------------\")\n",
    "    for cls, values in metrics.items():\n",
    "        print(f\"Class: {cls}\")\n",
    "        print(f\"  AP: {values['AP']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run evaluation\n",
    "def main(model_paths, dataset_path, ground_truths, class_names):\n",
    "    models = load_models(model_paths)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating Model: {model_name}\")\n",
    "\n",
    "        predictions, images = perform_inference(model, dataset_path)\n",
    "        metrics = evaluate_metrics(predictions, ground_truths, class_names)\n",
    "\n",
    "        visualize_metrics(metrics)\n",
    "        display_summary(metrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
